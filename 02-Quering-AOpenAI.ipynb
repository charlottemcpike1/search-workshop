{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Queries with and without Azure OpenAI"
      ],
      "metadata": {},
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you have your Search Engine loaded and in this notebook we are going to try some example queries and then use Azure OpenAI service to see if we can get a good answer for the user query.\n"
      ],
      "metadata": {},
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables"
      ],
      "metadata": {},
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from typing import List\n",
        "from operator import itemgetter\n",
        "\n",
        "# LangChain Imports needed\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "\n",
        "# Our own libraries needed\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "from common.utils import get_search_results\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198635513
        }
      },
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198636575
        }
      },
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Index Search queries"
      ],
      "metadata": {},
      "id": "9297d29b-1f61-4dce-858e-bf4272172dba"
    },
    {
      "cell_type": "code",
      "source": [
        "# Index that we are going to query (from Notebook 01)\n",
        "index_name = \"index-test\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198640392
        },
        "scrolled": true,
        "tags": []
      },
      "id": "5a46e2d3-298a-4708-83de-9e108b1a117a"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"What is a 21 day LBA?\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198641096
        }
      },
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: indexes must have a semantic configuration**.\n",
        "\n",
        "We are going to use Hybrid Queries: Text + Vector Search combined for optimal results!"
      ],
      "metadata": {},
      "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "\n",
        "search_payload = {\n",
        "    \"search\": QUESTION, # Text query\n",
        "    \"select\": \"id, jurisdiction, name, location, chunk\",\n",
        "    \"queryType\": \"semantic\",\n",
        "    \"vectorQueries\": [{\"text\": QUESTION, \"fields\": \"chunkVector\", \"kind\": \"text\", \"k\": k}], # Vector query\n",
        "    \"semanticConfiguration\": \"my-semantic-config\",\n",
        "    \"captions\": \"extractive\",\n",
        "    \"answers\": \"extractive\",\n",
        "    \"count\":\"true\",\n",
        "    \"top\": k\n",
        "}\n",
        "\n",
        "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search\",\n",
        "                    data=json.dumps(search_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "\n",
        "search_results = r.json()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198653200
        }
      },
      "id": "faf2e30f-e71f-4533-ab52-27d048b80a89"
    },
    {
      "cell_type": "code",
      "source": [
        "#r.text"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713185821418
        }
      },
      "id": "255c40f5-d836-480c-8c68-06a2282c8146"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the top results (from both searches) based on the score"
      ],
      "metadata": {
        "tags": []
      },
      "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389"
    },
    {
      "cell_type": "code",
      "source": [
        "#reranker_threshold = 1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713198732394
        }
      },
      "id": "234a7271-6f10-4382-bee6-2cb88e2f7afb"
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML('<h4>Top Answers</h4>'))\n",
        "\n",
        "for result in search_results['@search.answers']:\n",
        "    if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
        "        display(HTML('<h5>' + 'Answer - score: ' + str(round(result['score'],2)) + '</h5>'))\n",
        "        display(HTML(result['text']))\n",
        "            \n",
        "print(\"\\n\\n\")\n",
        "display(HTML('<h4>Top Results</h4>'))\n",
        "\n",
        "content = dict()\n",
        "ordered_content = OrderedDict()\n",
        "\n",
        "for result in search_results['value']:\n",
        "    if result['@search.rerankerScore'] > 1:# Show answers that are at least 25% of the max possible score=4\n",
        "        content[result['id']]={\n",
        "                                \"jurisdiction\": result['jurisdiction'],\n",
        "                                \"chunk\": result['chunk'], \n",
        "                                \"name\": result['name'], \n",
        "                                \"location\": result['location'] ,\n",
        "                                \"caption\": result['@search.captions'][0]['text'],\n",
        "                                \"score\": result['@search.rerankerScore'],\n",
        "                                \"index\": index_name\n",
        "                                }\n",
        "\n",
        "#After results have been filtered we will Sort and add them as an Ordered list\\n\",\n",
        "for id in sorted(content, key= lambda x: content[x][\"score\"], reverse=True):\n",
        "    ordered_content[id] = content[id]\n",
        "    url = str(ordered_content[id]['location']) + os.environ['BLOB_SAS_TOKEN']\n",
        "    title = str(ordered_content[id]['jurisdiction']) if (ordered_content[id]['jurisdiction']) else ordered_content[id]['name']\n",
        "    score = str(round(ordered_content[id]['score'],2))\n",
        "    display(HTML('<h5><a href=\"'+ url + '\">' + title + '</a> - score: '+ score + '</h5>'))\n",
        "    display(HTML(ordered_content[id]['caption']))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198735107
        }
      },
      "id": "9e938337-602d-4b61-8141-b8c92a5d91da"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments on Query results"
      ],
      "metadata": {},
      "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above the semantic re-ranking feature of Azure AI Search service is good. It gives answers (sometimes) and also the top results with the corresponding file and the paragraph where the answers is possible located.\n",
        "\n",
        "Let's see if we can make this better with Azure OpenAI"
      ],
      "metadata": {},
      "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Azure OpenAI\n",
        "\n",
        "To use OpenAI to get a better answer to our question, the thought process is simple: let's **give the answer and the content of the documents from the search result to the GPT model as context and let it provide a better response**. This is what RAG (Retreival Augmented Generation) is about.\n",
        "\n",
        "Now, before we do this, we need to understand a few things first:\n",
        "\n",
        "1) Chainning and Prompt Engineering\n",
        "2) Embeddings\n",
        "\n",
        "We will use a library call **LangChain** that wraps a lot of boiler plate code.\n",
        "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
      ],
      "metadata": {},
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198750139
        }
      },
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note**: Starting now, we will utilize OpenAI models. Please ensure that you have deployed the following models within the Azure OpenAI portal:\n",
        "\n",
        "- text-embedding-ada-002 (or newer)\n",
        "- gpt-35-turbo (1106 or newer)\n",
        "- gpt-4-turbo (1106 or newer)\n",
        "\n",
        "Reference for Azure OpenAI models (regions, limits, dimensions, etc): [HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)"
      ],
      "metadata": {},
      "id": "325d9138-2250-4f6b-bc88-50d7957f8d33"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A gentle intro to chaining LLMs and prompt engineering"
      ],
      "metadata": {},
      "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.\n",
        "\n",
        "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
        "\n",
        "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
        "\n",
        "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
        "\n",
        "Here’s an example:"
      ],
      "metadata": {},
      "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69"
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETION_TOKENS = 500\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0, \n",
        "                      max_tokens=COMPLETION_TOKENS)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198756267
        }
      },
      "id": "13df9247-e784-4e04-9475-55e672efea47"
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}.\")\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198758043
        }
      },
      "id": "a3b55adb-6f98-4f15-b67a-9fbba5820560"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component."
      ],
      "metadata": {},
      "id": "6417d052-0035-4635-93e8-2bd3ec50d796"
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198761272
        }
      },
      "id": "77a37e60-a1ef-4750-a1ec-9e4fe5ba07fa"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "display(Markdown(chain.invoke({\"input\": QUESTION})))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "6be6b4df-ee2c-4a0c-8ad3-a672d70f4f8d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: this is the first time you use OpenAI in this Accelerator, so if you get a Resource not found error, is most likely because the name of your OpenAI model deployment is different than the environmental variable set above `os.environ[\"GPT35_DEPLOYMENT_NAME\"]`"
      ],
      "metadata": {},
      "id": "cd8539d0-a538-4368-82c3-5f91d8370f1e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
        "\n",
        "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
      ],
      "metadata": {},
      "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The second type of Chains are Utility:**\n",
        "\n",
        "* Utility — These are specialized chains, comprised of many building blocks to help solve a specific task. For example, LangChain supports some end-to-end chains (such as `create_retrieval_chain` for QnA Doc retrieval, Summarization, etc).\n",
        "\n",
        "We will build our own specific chain in this workshop for digging deeper and solve our use case of enhancing the results of Azure AI Search."
      ],
      "metadata": {
        "tags": []
      },
      "id": "12c48038-b1af-4228-8ffb-720e554fd3b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "But before dealing with the utility chain needed, let's first review the concept of Embeddings and Vector Search and RAG. \n",
        "\n",
        "## Embeddings and Vector Search\n",
        "\n",
        "From the Azure OpenAI documentation ([HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=python)), An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar. \n",
        "\n",
        "### Why Do We Need Vectors?\n",
        "\n",
        "Vectors are essential for several reasons:\n",
        "\n",
        "- **Semantic Richness**: They convert the semantic meaning of text into mathematical vectors, capturing nuances that simple keyword searches miss. This makes them incredibly powerful for understanding and processing language.\n",
        "- **Human-like Searching**: Searching using vector distances mimics the human approach to finding information based on context and meaning, rather than relying solely on exact word matches.\n",
        "- **Efficiency in Scale**: Vector representations allow for efficient handling and searching of large datasets. By reducing complex text to numerical vectors, algorithms can quickly sift through vast amounts of information.\n",
        "\n",
        "### Understanding LLM Tokens' Context Limitation\n",
        "\n",
        "Large Language Models (LLMs) like GPT come with a token limit for each input, which poses a challenge when dealing with lengthy documents or extensive data sets. This limitation restricts the model's ability to understand and generate responses based on the full context of the information provided. It becomes crucial, therefore, to devise strategies that can effectively manage and circumvent this limitation to leverage the full power of LLMs.\n",
        "\n",
        "To address this challenge, the solution incorporates several key steps:\n",
        "\n",
        "1. **Segmenting Documents**: Breaking down large documents into smaller, manageable segments.\n",
        "2. **Vectorization of Chunks**: Converting these segments into vectors, making them compatible with vector-based search techniques.\n",
        "3. **Hybrid Search**: Employing both vector and text search methods to pinpoint the most relevant segments in relation to the query.\n",
        "4. **Optimal Context Provision**: Presenting the LLM with the most pertinent segments, ensuring a balance between detail and brevity to stay within token limits.\n",
        "\n",
        "\n",
        "Our ultimate goal is to rely solely on vector indexes and hybrid searchs (vector + text). While it is possible to manually code parsers with OCR for various file types and develop a scheduler to synchronize data with the index, there is a more efficient alternative: **Azure AI Search has automated chunking strategies and vectorization**.\n",
        "\n",
        "It's important to note that **document segmentation and vectorization have already been completed in AI Azure Search**, as seen in the `ordered_content` dictionary. This pre-processing step simplifies subsequent operations, ensuring rapid response times and adherence to the token limits of the chosen OpenAI model.\n"
      ],
      "metadata": {},
      "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So really, our only job now is to make sure that the results from the Azure AI Search queries fit on the LLM context size, and then let it do its magic."
      ],
      "metadata": {},
      "id": "80e79235-3d8b-4713-9336-5004cc4a1556"
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"cogsrch-index-files4\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198775889
        }
      },
      "id": "12682a1b-df92-49ce-a638-7277103f6cb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not duplicate code, we have put many of the code used above into functions. These functions are in the `common/utils.py` and `common/prompts.py` files. This way we can use these functions in the app that we will build later.\n"
      ],
      "metadata": {},
      "id": "78a6d6a7-18ef-45b2-a216-3c1f50006593"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_results(query: str, indexes: str, \n",
        "                       k: int = 5,\n",
        "                       reranker_threshold: int = 1,\n",
        "                       sas_token: str = \"\") -> List[dict]:\n",
        "    \"\"\"Performs hybrid search and returns ordered dictionary with the results\"\"\"\n",
        "    \n",
        "    headers = {'Content-Type': 'application/json','api-key': os.environ[\"AZURE_SEARCH_KEY\"]}\n",
        "    params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}\n",
        "\n",
        "    search_payload = {\n",
        "        \"search\": query,\n",
        "        \"select\": \"id, jurisdiction, chunk, name, location\",\n",
        "        \"queryType\": \"semantic\",\n",
        "        \"vectorQueries\": [{\"text\": query, \"fields\": \"chunkVector\", \"kind\": \"text\", \"k\": k}],\n",
        "        \"semanticConfiguration\": \"my-semantic-config\",\n",
        "        \"captions\": \"extractive\",\n",
        "        \"answers\": \"extractive\",\n",
        "        \"count\":\"true\",\n",
        "        \"top\": k    \n",
        "    }\n",
        "\n",
        "    resp = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search\",\n",
        "                    data=json.dumps(search_payload), headers=headers, params=params)\n",
        "\n",
        "    search_results = resp.json()\n",
        "    \n",
        "    content = dict()\n",
        "    ordered_content = OrderedDict()\n",
        "    \n",
        "    for result in search_results['value']:\n",
        "        if result['@search.rerankerScore'] > reranker_threshold: # Show results that are at least N% of the max possible score=4\n",
        "            content[result['id']]={\n",
        "                                    \"jurisdiction\": result['jurisdiction'], \n",
        "                                    \"name\": result['name'], \n",
        "                                    \"chunk\": result['chunk'],\n",
        "                                    \"location\": result['location'] + sas_token if result['location'] else \"\",\n",
        "                                    \"caption\": result['@search.captions'][0]['text'],\n",
        "                                    \"score\": result['@search.rerankerScore'],\n",
        "                                    \"index\": index_name\n",
        "                                }\n",
        "            \n",
        "\n",
        "    topk = k\n",
        "        \n",
        "    count = 0  # To keep track of the number of results added\n",
        "    for id in sorted(content, key=lambda x: content[x][\"score\"], reverse=True):\n",
        "        ordered_content[id] = content[id]\n",
        "        count += 1\n",
        "        if count >= topk:  # Stop after adding topK results\n",
        "            break\n",
        "\n",
        "    return ordered_content\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1713198835439
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "96a12553-2b32-4f1f-872a-9090fd9a1604"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3  # play with this parameter and see the quality of the final answer\n",
        "ordered_results = get_search_results(QUESTION, index_name, k=k, reranker_threshold=1)\n",
        "print(\"Number of results:\",len(ordered_results))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198839975
        }
      },
      "id": "3bccca45-d1dd-476f-b109-a528b857b6b3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the below line if you want to inspect the ordered results\n",
        "#ordered_results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713180868732
        }
      },
      "id": "7714f38a-daaa-4fc5-a95a-dd025d153216"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a Prompt Template that will ground the response only in the chunks retrieve by our hybrid AI Search."
      ],
      "metadata": {},
      "id": "235d4238-df6e-40eb-ab38-4fe6db614acd"
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question thoroughly, based **ONLY** on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198847522
        }
      },
      "id": "f86ed786-aca0-4e25-947b-d9cf3a82665c"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# Creation of our custom chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "try:\n",
        "    display(Markdown(chain.invoke({\"question\": QUESTION, \"context\": ordered_results})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "25cba3d1-b5ab-4e28-96b3-ef923d99dc9f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From GPT-3.5 to GPT-4\n",
        "\n",
        "Now let's see how the response changes if we change to GPT-4\n"
      ],
      "metadata": {},
      "id": "406c2b91-6752-4ea2-b95c-d1a52dbdd62b"
    },
    {
      "cell_type": "code",
      "source": [
        "llm_2 = AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0, max_tokens=COMPLETION_TOKENS)\n",
        "chain = prompt | llm_2 | output_parser"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198857159
        }
      },
      "id": "f01705f1-7194-452b-a170-c09ba7752b1d"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "try:\n",
        "    display(Markdown(chain.invoke({\"question\": QUESTION, \"context\": ordered_results})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "4c83bb17-36d3-4eb6-ae6f-9c68f3033d2b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As we can see, the model selection MATTERS!\n",
        "\n",
        "We will dive deeper into this later, but for now, **look at the diference between GPT3.5 and GPT4, in quality and in response time**."
      ],
      "metadata": {},
      "id": "92a28869-13c1-463e-8285-9870e6ac1946"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the Prompt and adding citations\n",
        "\n",
        "We could see above that the answer given by GPT3.5 was very simple compared to GPT4, even when the prompt says \"thorough responses to users\". We also could see that there is no citations or references. **How do we know if the answer is grounded on the context or not?**\n",
        "\n",
        "Let's see if these two issues can be improved by Prompt Engineering.<br>\n",
        "On `common/prompts.py` we created a prompt called `DOCSEARCH_PROMPT` check it out!\n",
        "\n",
        "Let's also create a custom Retriever class so we can plug it in easily within the chain building. \n",
        "Note: we can also use the Azure AI Search retriever class [HERE](https://python.langchain.com/docs/integrations/vectorstores/azuresearch), however we want to create a custom Retriever for the following reasons:\n",
        "\n",
        "1) We want to do multi-index searches in one call\n",
        "2) Easier to teach complex concepts of LangChain in this notebook\n",
        "3) We want to use the REST API vs the Python Azure Search SDK"
      ],
      "metadata": {},
      "id": "417925af-486a-40bc-a290-28c35968c581"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRetriever(BaseRetriever):\n",
        "    \n",
        "    topK : int\n",
        "    reranker_threshold : int\n",
        "    indexes: str\n",
        "    sas_token: str = None\n",
        "    \n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \n",
        "        ordered_results = get_search_results(query, self.indexes, k=self.topK, \n",
        "                                             reranker_threshold=self.reranker_threshold, \n",
        "                                             sas_token=self.sas_token)\n",
        "        top_docs = []\n",
        "        for key,value in ordered_results.items():\n",
        "            location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
        "            top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location, \"score\":value[\"score\"]}))\n",
        "\n",
        "        return top_docs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198908202
        }
      },
      "id": "bdf31f99-0dfb-423a-81f5-03018e61d9a9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the retriever\n",
        "retriever = CustomRetriever(indexes=index_name, topK=k, reranker_threshold=1, sas_token=os.environ['BLOB_SAS_TOKEN'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198908420
        }
      },
      "id": "19b39c79-c827-4437-b58b-6a6fae53b968"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test retreiver\n",
        "results = retriever.get_relevant_documents(QUESTION)\n",
        "len(results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198908725
        }
      },
      "id": "c7aa4f58-4791-40a0-80c5-6582e0574579"
    },
    {
      "cell_type": "code",
      "source": [
        "# We can create now a dynamically configurable llm object that can change the model at runtime\n",
        "dynamic_llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                              temperature=0, max_tokens=COMPLETION_TOKENS).configurable_alternatives(\n",
        "    # This gives this field an id\n",
        "    # When configuring the end runnable, we can then use this id to configure this field\n",
        "    ConfigurableField(id=\"model\"),\n",
        "    # This sets a default_key.\n",
        "    # If we specify this key, the default LLM  (initialized above) will be used\n",
        "    default_key=\"gpt35\",\n",
        "    # This adds a new option, with name `gpt4`\n",
        "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], \n",
        "                         temperature=0.5, max_tokens=COMPLETION_TOKENS),\n",
        "    # You can add more configuration options here\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198908945
        }
      },
      "id": "11b6546f-b5c5-4168-97fc-2636c50e41c2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaration of the chain with the dynamic llm and the new prompt\n",
        "configurable_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | DOCSEARCH_PROMPT  # Passes the input variables above to the prompt template\n",
        "    | dynamic_llm   # Passes the finished prompt to the LLM\n",
        "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713198909124
        }
      },
      "id": "d7da2f31-cf5d-4f3a-aad5-67b50b56968e"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "try:\n",
        "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt35\"}).invoke({\"question\": QUESTION})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "b67200e5-d3ae-4c86-9f69-bc7b964ab532"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, we were able to improve the quality and breath of the answer and add citations with only prompt engineering!\n",
        "\n",
        "Let's try again GPT-4"
      ],
      "metadata": {},
      "id": "8661b48d-1e57-4a70-9b0a-cc59f9093267"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "try:\n",
        "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt4\"}).invoke({\"question\": QUESTION})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "efcfac6b-bac2-40c6-9ded-e4ee38e3093f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As you can see the answer from GPT4 is richer, and includes all the relevant chunks. GPT3.5 tends to focus in the first and last chunks only"
      ],
      "metadata": {},
      "id": "c93b000a-8d0a-4574-be7c-48d26dfb4c70"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Streaming to improve user experience and performance\n",
        "\n",
        "It is obvious by now that **GPT4 answers are better quality than GPT3.5**. None are incorrect, but GPT4 is better at understanding the context, following the prompt instructions and on giving a comprehensive answer.\n",
        "\n",
        "One way to make GPT4 look faster is to stream the answer, so the user can see the response as it is typed. To do this, we just simply need to call the method `stream` instead of `invoke`. More on Streaming and Callbacks in later notebooks, but for now, this is one simple way to do it:"
      ],
      "metadata": {},
      "id": "6690453b-a9b1-4907-bd43-8c6b3ecba26e"
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in configurable_chain.with_config(configurable={\"model\": \"gpt4\"}).stream({\"question\": QUESTION}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713199118437
        }
      },
      "id": "6d250c88-5984-438f-8390-1d93756048ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "##### By using OpenAI, the answers to user questions are way better than taking just the results from Azure AI Search. So the summary is:\n",
        "- Utilizing Azure AI Search, we conduct a multi-index hybrid search that identifies the top chunks of documents from each index.\n",
        "- Subsequently, Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.\n",
        "- Best of two worlds!\n",
        "\n",
        "##### Important observations on this notebook:\n",
        "\n",
        "1) Answers with GPT-3.5 are less quality but way faster\n",
        "2) Answers with GPT-3.5 sometimes failed on provinding citations in the right format\n",
        "3) Answers with GPT-4 are great quality but way slower\n",
        "4) Answers with GPT-4 always provide good and diverse citations in the right format\n",
        "5) Streaming the answers improves the user experience big time!"
      ],
      "metadata": {},
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
      ],
      "metadata": {},
      "id": "fdc6e2fe-1c34-4952-99ad-14940f022379"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}